{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prediction: I predict that the logistic regression will do better compared to random forest because this data output seems fairly linear to me, therefore logistic regression seems very fitting with our objective"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turning csv's into DataFrames\n",
    "train_df = pd.read_csv(Path('Resources/2019loans.csv'))\n",
    "test_df = pd.read_csv(Path('Resources/2020Q1loans.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping the column that I want to predict on the training data\n",
    "X_train = train_df.drop('loan_status',axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating training data X and Y\n",
    "X_dummies = pd.get_dummies(X_train)\n",
    "y_train_label = LabelEncoder().fit_transform(train_df['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating testing data X and Y\n",
    "X_test = test_df.drop('loan_status',axis = 1)\n",
    "X_dummies_test = pd.get_dummies(X_test).reindex(columns=X_dummies.columns,fill_value=0)\n",
    "y_test_label = LabelEncoder().fit_transform(test_df['loan_status'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Performing Logistic Regression on unscaled data\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "classifier = LogisticRegression()\n",
    "classifier.fit(X_dummies, y_train_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.6485221674876848\n",
      "Testing Data Score: 0.5253083794130158\n"
     ]
    }
   ],
   "source": [
    "# Printing our Training and Testing Score\n",
    "print(f\"Training Data Score: {classifier.score(X_dummies, y_train_label)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_dummies_test, y_test_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Actual</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4697</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4698</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4699</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4700</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4701</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4702 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Prediction  Actual\n",
       "0              1       1\n",
       "1              1       1\n",
       "2              1       1\n",
       "3              0       1\n",
       "4              1       1\n",
       "...          ...     ...\n",
       "4697           1       0\n",
       "4698           1       0\n",
       "4699           1       0\n",
       "4700           1       0\n",
       "4701           0       0\n",
       "\n",
       "[4702 rows x 2 columns]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = classifier.predict(X_dummies_test)\n",
    "pd.DataFrame({\"Prediction\": predictions, \"Actual\": y_test_label})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.56      0.22      0.32      2351\n",
      "           1       0.52      0.83      0.64      2351\n",
      "\n",
      "    accuracy                           0.53      4702\n",
      "   macro avg       0.54      0.53      0.48      4702\n",
      "weighted avg       0.54      0.53      0.48      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test_label\n",
    "y_pred = classifier.predict(X_dummies_test)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5253083794130158\n"
     ]
    }
   ],
   "source": [
    "# Finding our accuracy on unscaled logistic regression\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6180348787749894\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model and print the model score\n",
    "# Train a Random Forest Classifier model and print the model score\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_dummies, y_train_label)\n",
    "print(f'Training Score: {clf.score(X_dummies, y_train_label)}')\n",
    "print(f'Testing Score: {clf.score(X_dummies_test, y_test_label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 780, 1571],\n",
       "       [ 225, 2126]], dtype=int64)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test_label\n",
    "y_pred = clf.predict(X_dummies_test)\n",
    "confusion_matrix(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.78      0.33      0.46      2351\n",
      "           1       0.58      0.90      0.70      2351\n",
      "\n",
      "    accuracy                           0.62      4702\n",
      "   macro avg       0.68      0.62      0.58      4702\n",
      "weighted avg       0.68      0.62      0.58      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "pd.DataFrame({\"Prediction\": y_pred, \"Actual\": y_test_label})\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6180348787749894\n"
     ]
    }
   ],
   "source": [
    "# Finding our accuracy on unscaled random forest classifier\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With a near 62% accuracy from random forest classifier compared to a 52% accuracy from logistic regression, random forest classifier is noticably more accurate on unscaled data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Score: 0.6386699507389163\n",
      "Testing Data Score: 0.5070182900893236\n"
     ]
    }
   ],
   "source": [
    "# Scale the data\n",
    "scaler = StandardScaler().fit(X_dummies)\n",
    "X_train_scaled = scaler.transform(X_dummies)\n",
    "X_test_scaled = scaler.transform(X_dummies_test)\n",
    "print(f\"Training Data Score: {classifier.score(X_train_scaled, y_train_label)}\")\n",
    "print(f\"Testing Data Score: {classifier.score(X_test_scaled, y_test_label)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# With scaled data, I would still maintain that logistic regression will do better. This time, it's because knowing that logistic regression does better with data scaling compared to random forest, which does not respond well to scaling. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.51      0.24      0.33      2351\n",
      "           1       0.50      0.77      0.61      2351\n",
      "\n",
      "    accuracy                           0.51      4702\n",
      "   macro avg       0.51      0.51      0.47      4702\n",
      "weighted avg       0.51      0.51      0.47      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Train the Logistic Regression model on the scaled data\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test_label\n",
    "y_pred = classifier.predict(X_test_scaled)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Score: 1.0\n",
      "Testing Score: 0.6193109315185028\n"
     ]
    }
   ],
   "source": [
    "# Train a Random Forest Classifier model on the scaled data\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(random_state=1, n_estimators=500).fit(X_train_scaled, y_train_label)\n",
    "print(f'Training Score: {clf.score(X_train_scaled, y_train_label)}')\n",
    "print(f'Testing Score: {clf.score(X_test_scaled, y_test_label)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.5070182900893236\n"
     ]
    }
   ],
   "source": [
    "# Finding our accuracy on scaled logistic regression\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.44      0.67      0.53      2351\n",
      "           1       0.33      0.16      0.22      2351\n",
      "\n",
      "    accuracy                           0.42      4702\n",
      "   macro avg       0.39      0.42      0.37      4702\n",
      "weighted avg       0.39      0.42      0.37      4702\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "y_true = y_test_label\n",
    "y_pred = clf.predict(X_dummies_test)\n",
    "confusion_matrix(y_true, y_pred)\n",
    "\n",
    "tn, fp, fn, tp = confusion_matrix(y_true, y_pred).ravel()\n",
    "accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.415142492556359\n"
     ]
    }
   ],
   "source": [
    "# Finding our accuracy on scaled logistic regression\n",
    "print(f\"Accuracy: {accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For scaled data, logistic regression yieled an accuracy score of 50% compared to random forest's accuracy score of 41%"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
